{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9647,"status":"ok","timestamp":1666980888328,"user":{"displayName":"Romain Jean-Marc","userId":"07018817223688850984"},"user_tz":420},"id":"cO7Y9kB0ZiBn","outputId":"189bacdf-5598-4ad4-98fb-7b1c1bff6c02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"8HukWCm-GWWg"},"source":["https://buomsoo-kim.github.io/colab/2020/05/09/Colab-mounting-google-drive.md/"]},{"cell_type":"markdown","metadata":{"id":"2XiPaT9JGcEd"},"source":["NOte : Only use those lines of code if you import it in Google Colaboratory !"]},{"cell_type":"markdown","metadata":{"id":"wAwWc-RNGeID"},"source":["# GROUP PROJECT  - WEBSCRAPPING PHASE 1 - SCRAPPING DATA & CREATE A CSV FILE THAT CONTAINS THE DATA ABOUT REAL ESTATE\n","\n","real estate link \n","\n","https://www.realtor.com/realestateandhomes-search/Portland_OR  \n","\n","complete guide for web scraping \n","\n","https://www.webscrapingapi.com/real-estate-web-scraper\n","\n","https://medium.com/rakesh-nain/using-python-collecting-real-world-data-by-web-scraping-real-estate-website-and-doing-data-2feb50a3b94f\n","\n","getting an API proxi \n","\n","https://scrapeops.io/app/proxy\n","https://app.webscrapingapi.com/1/dashboardService\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hY1Y157eHxDU"},"source":["## 1 ) Preparing the work space : \n"," \n"," importing the module we are going to use : beautifulsoup, pandas, requests\n","\n"," https://www.webscrapingapi.com/real-estate-web-scraper#prepare-the-workspace"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veGWw2otYi5Q"},"outputs":[],"source":["# importing the BS4 and requests modules\n","from bs4 import BeautifulSoup \n","import pandas as pd\n","import requests"]},{"cell_type":"markdown","metadata":{"id":"GIq6s4tQIKC9"},"source":["## 2) Declaring the variables\n","\n"," - here we have 6 variables : **1 integer** (page) take the page number to integrate in the url of the real estate website. Note that for obvious reasons we will instantiate the value of the variable page as 1 for the first page.\n","\n"," - **5** variables(prices, beds, baths, sizes, and addresses) that we want to fetch from the real estate website. Note that those variables are of **list type** which is practicle to transfert on a csv file later on using the Pandas library. Those variables will be later on in the code populating by the values that we fetch on the website."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtkyDRsZYrB6"},"outputs":[],"source":["page = 1\n","# column for the csv file and variable of type list, we will use pandas to transfer into the file\n","prices = []\n","beds = []\n","baths = []\n","sizes = []\n","addresses = []\n"]},{"cell_type":"markdown","metadata":{"id":"zBevdNHdIK1l"},"source":["## 3) Looping through the pages\n","\n","1. we begin the loop by telling python to execute the loop as long as the variable page has not a value of 86. Why 86 ? because there are 86 pages on the real estate website. \n","2. The **'html.parser**' tells python that the website content is written in HTML.\n","3. Once we have the content of the page in the variable soup, we create another variable labeled lists, we populate the variable by using the function find_all from Beautifulsoup. We give to to the function two parameters : the **class parameter** of the element we want to fetch('**li**' class here) which is the 'house card' , each house card contains all the element we wants (price, address, number of bed, number of bathrooms, size). and the second element we give to the function '**find_all**'. why do we use the find all ? because there is more that one house in a page. the second parameter is the name of the section. We past the html label from the source code of the real estate page.\n","4. once we have a house we start another loop (nested for loop) we ask python to use the find function to fetch all the elements that we need. we don't use the functon find all because for each house there is only one price, one set of bedrooms, bathrooms, size, one address. \n","5. the function find used to parse each elements of the house take some parameters such as the class of the element and the label. we ask python to find one elemnt with a certain class who is labelled a certain way. \n","6. At the end of the loop, we update the number of the page. For instance, after the first loop, the number of the page will havce a value of 2 which will change the url of the webpage. the second page of the real estate website will be send in a new request and cycle to fetch houses. \n","\n","\n","multi page scraping \n","https://www.geeksforgeeks.org/how-to-scrape-multiple-pages-of-a-website-using-python/\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bZfDUshGCdc"},"outputs":[],"source":["while page!=86:\n","    # our API proxi that is sent with our request \n","    url = \"https://api.webscrapingapi.com/v1\"\n","    params = {\"api_key\": \"eBiY1EnI2X1RnBFG7mYXpoNhdmrLLhPN\",\"url\": f\"https://www.realtor.com/realestateandhomes-search/Portland_OR/pg-{page}\"}\n","    res = requests.request(\"GET\", url, params=params)\n","\n","    # beautiful soup fetch the content of the page in a variable soup\n","    soup = BeautifulSoup(res.content, 'html.parser')\n","\n","    lists = soup.find_all('li', class_='jsx-1881802087 component_property-card')\n","\n","    for element in lists:\n","      price = element.find('span', attrs={'data-label': 'pc-price'})\n","      bed = element.find('li', attrs={'data-label': 'pc-meta-beds'})\n","      bath = element.find('li', attrs={'data-label': 'pc-meta-baths'})\n","      size = element.find('li', attrs={'data-label': 'pc-meta-sqft'})\n","      address = element.find('div', attrs={'data-label': 'pc-address'})\n","      if bed and bath:\n","        nr_beds = bed.find('span', attrs={'data-label': 'meta-value'})\n","        nr_baths = bath.find('span', attrs={'data-label': 'meta-value'})\n","        if nr_beds and float(nr_beds.text) >= 2 and nr_baths and float(nr_baths.text) >= 1:\n","           beds.append(nr_beds.text)\n","           baths.append(nr_baths.text)\n","\n","           if price and price.text:\n","               prices.append(price.text)\n","           else:\n","               prices.append('No display data')\n","\n","           if size and size.text:\n","               sizes.append(size.text)\n","           else:\n","               sizes.append('No display data')\n","\n","           if address and address.text:\n","               addresses.append(address.text)\n","           else:\n","               addresses.append('No display data')  \n","          \n","    page = page + 1\n"]},{"cell_type":"markdown","metadata":{"id":"okjvtC_vSuz9"},"source":["## 3) copy the data into a csv file using the pandas library.\n","\n","The elemnents that we have fetched have populated the 5 variables of list type mentioned in the second step of that report. WE create the column name and we associate each column with the variable ad hoc. the second line of the code create a file name 'listing.csv' that we eill use later on"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_9Dsk-oME2V"},"outputs":[],"source":["df = pd.DataFrame({'Address': addresses, 'Price': prices, 'Beds': beds, 'Baths': baths, 'Sizes': sizes})\n","df.to_csv('listings.csv', index=False, encoding='utf-8')   \n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOQxo5W6cBkDJ2DTSFqqQGi","collapsed_sections":[],"mount_file_id":"1tmcPwV-b6LPWuYZm65TQ6w36mxi_skbA","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4"},"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}}},"nbformat":4,"nbformat_minor":0}
